{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225cf363-d264-4ede-8cdf-3e76e45879b4",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52388d-69c6-4316-8f38-052f3e812dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f755c-c721-4610-a9b8-88d9ccc75d7e",
   "metadata": {},
   "source": [
    "## Fetch image from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51980ced-d367-4f9e-9e6e-6b496cb63014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astilbe folder has 726 images\n",
      "bellflower folder has 872 images\n",
      "black_eyed_susan folder has 986 images\n",
      "calendula folder has 1011 images\n",
      "california_poppy folder has 1021 images\n",
      "carnation folder has 924 images\n",
      "common_daisy folder has 978 images\n",
      "coreopsis folder has 1035 images\n",
      "dandelion folder has 1038 images\n",
      "iris folder has 1041 images\n",
      "rose folder has 986 images\n",
      "sunflower folder has 1013 images\n",
      "tulip folder has 1034 images\n",
      "water_lily folder has 977 images\n",
      "\n",
      "\n",
      "train folder has 13642 images\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dirs = os.listdir(\"train/\")\n",
    "for dir in dirs:\n",
    "    files = list(os.listdir(\"train/\" + dir))\n",
    "    print( dir + \" folder has \" + str(len(files)) + \" images\")\n",
    "    count = count + len(files)\n",
    "\n",
    "print(\"\\n\\ntrain folder has \" + str(count) + \" images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d591010-3fd1-4291-8264-f80b259ac047",
   "metadata": {},
   "source": [
    "## Load images into arrays as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef67f7d-21f1-4759-b029-d09790e59d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"train/\"\n",
    "img_size = 256\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90be8d8-610d-422f-9411-d8e8570e29bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13642 files belonging to 14 classes.\n",
      "Using 10914 files for training.\n",
      "Found 13642 files belonging to 14 classes.\n",
      "Using 2728 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory( train_dir,\n",
    "                                                       seed = 123,\n",
    "                                                       validation_split = 0.2,\n",
    "                                                       subset = \"training\",\n",
    "                                                       batch_size = batch,\n",
    "                                                       image_size = (img_size, img_size))\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory( train_dir,\n",
    "                                                       seed = 123,\n",
    "                                                       validation_split = 0.2,\n",
    "                                                       subset = \"validation\",\n",
    "                                                       batch_size = batch,\n",
    "                                                       image_size = (img_size, img_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728de8a4-ba2b-4b27-b486-8d5304b2d8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astilbe',\n",
       " 'bellflower',\n",
       " 'black_eyed_susan',\n",
       " 'calendula',\n",
       " 'california_poppy',\n",
       " 'carnation',\n",
       " 'common_daisy',\n",
       " 'coreopsis',\n",
       " 'dandelion',\n",
       " 'iris',\n",
       " 'rose',\n",
       " 'sunflower',\n",
       " 'tulip',\n",
       " 'water_lily']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flower_names = train_ds.class_names\n",
    "flower_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad38bb0-b461-416a-bfac-4262d863c1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4b08f-5691-460e-a37e-117d45a863ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(flower_names[labels[i]])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac04547c-7586-4f32-a038-d21c90963408",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902f3cd5-cc61-48de-9c8d-06b575ea2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71ba8c1-59b4-4292-b317-5281d505e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67fb61-4008-4b1f-94d1-0c577c477d7e",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4e53c9-7463-4e6e-a748-05648a98acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", input_shape = (img_size, img_size, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e3fb09-1f16-445c-937f-5bd73230f8b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# i = 0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# i = 0\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        images = data_augmentation(images)\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a412286-dc48-4e3b-9cb5-6ad7d6e8f571",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd237ce-ad74-4ac7-a5d9-79ceff2d42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    Conv2D(16, 3, padding = \"same\", activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding = \"same\", activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding = \"same\", activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(14)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0602b94-610e-4cbd-9134-54c58ea71d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcfad9a2-02e6-48b7-8c6e-e93709cf85ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "rescaling (Rescaling)        (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8388736   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 8,414,126\n",
      "Trainable params: 8,414,126\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be17fc1-5c78-4b5e-9067-6bd53b0f4cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "342/342 [==============================] - 104s 193ms/step - loss: 1.7584 - accuracy: 0.3898 - val_loss: 1.4576 - val_accuracy: 0.4974\n",
      "Epoch 2/15\n",
      "342/342 [==============================] - 68s 198ms/step - loss: 1.3324 - accuracy: 0.5392 - val_loss: 1.2041 - val_accuracy: 0.5883\n",
      "Epoch 3/15\n",
      "342/342 [==============================] - 50s 147ms/step - loss: 1.1255 - accuracy: 0.6133 - val_loss: 1.0783 - val_accuracy: 0.6397\n",
      "Epoch 4/15\n",
      "342/342 [==============================] - 41s 120ms/step - loss: 1.0057 - accuracy: 0.6551 - val_loss: 1.0377 - val_accuracy: 0.6617\n",
      "Epoch 5/15\n",
      "342/342 [==============================] - 40s 118ms/step - loss: 0.9136 - accuracy: 0.6886 - val_loss: 0.9988 - val_accuracy: 0.6767\n",
      "Epoch 6/15\n",
      "342/342 [==============================] - 41s 121ms/step - loss: 0.8432 - accuracy: 0.7118 - val_loss: 0.9467 - val_accuracy: 0.6888\n",
      "Epoch 7/15\n",
      "342/342 [==============================] - 39s 114ms/step - loss: 0.7770 - accuracy: 0.7326 - val_loss: 0.9491 - val_accuracy: 0.6987\n",
      "Epoch 8/15\n",
      "342/342 [==============================] - 41s 120ms/step - loss: 0.7255 - accuracy: 0.7558 - val_loss: 0.8519 - val_accuracy: 0.7221\n",
      "Epoch 9/15\n",
      "342/342 [==============================] - 45s 132ms/step - loss: 0.6801 - accuracy: 0.7704 - val_loss: 0.8699 - val_accuracy: 0.7199\n",
      "Epoch 10/15\n",
      "342/342 [==============================] - 46s 134ms/step - loss: 0.6667 - accuracy: 0.7749 - val_loss: 0.8130 - val_accuracy: 0.7342\n",
      "Epoch 11/15\n",
      "342/342 [==============================] - 42s 124ms/step - loss: 0.6148 - accuracy: 0.7888 - val_loss: 0.8154 - val_accuracy: 0.7493\n",
      "Epoch 12/15\n",
      "342/342 [==============================] - 38s 113ms/step - loss: 0.5897 - accuracy: 0.7993 - val_loss: 0.8867 - val_accuracy: 0.7247\n",
      "Epoch 13/15\n",
      "342/342 [==============================] - 42s 124ms/step - loss: 0.5662 - accuracy: 0.8069 - val_loss: 0.8263 - val_accuracy: 0.7386\n",
      "Epoch 14/15\n",
      "342/342 [==============================] - 41s 119ms/step - loss: 0.5377 - accuracy: 0.8157 - val_loss: 0.9287 - val_accuracy: 0.7287\n",
      "Epoch 15/15\n",
      "342/342 [==============================] - 41s 119ms/step - loss: 0.5078 - accuracy: 0.8281 - val_loss: 0.7781 - val_accuracy: 0.7562\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs = 15, validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e16165-4b6c-4e7b-9489-a97d073f04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(image_path):\n",
    "    input_image = tf.keras.utils.load_img(image_path, target_size=(256,256))\n",
    "    input_image_array = tf.keras.utils.img_to_array(input_image)\n",
    "    input_image_exp_dim = tf.expand_dims(input_image_array, 0)\n",
    "    \n",
    "    predictions = model.predict(input_image_exp_dim)\n",
    "    result = tf.nn.softmax(predictions[0])\n",
    "    outcome = \"The image belongs to \" + flower_names[np.argmax(result)] + \" with a score of \" + str(max(result) * 100)\n",
    "    return flower_names[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eaf22b8-69dd-48b0-8d85-2a566cc4e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Flower_Recognition_Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d2fe8-6e8c-4ae9-9871-9b6b9a5aed97",
   "metadata": {},
   "source": [
    "## Load Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da145392-5d09-4114-a035-f573fe958b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Flower_Recognition_Model.h5')\n",
    "flower_names = ['astilbe',\n",
    " 'bellflower',\n",
    " 'black_eyed_susan',\n",
    " 'calendula',\n",
    " 'california_poppy',\n",
    " 'carnation',\n",
    " 'common_daisy',\n",
    " 'coreopsis',\n",
    " 'dandelion',\n",
    " 'iris',\n",
    " 'rose',\n",
    " 'sunflower',\n",
    " 'tulip',\n",
    " 'water_lily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf0c34a-9594-4836-a921-322ff12ef751",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m countTotalFileTest \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m countTotalCorrect \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m dirs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m      5\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "countTotalFileTest = 0\n",
    "countTotalCorrect = 0\n",
    "dirs = os.listdir(\"test/\")\n",
    "for dir in dirs:\n",
    "    files = list(os.listdir(\"test/\" + dir))\n",
    "    for file in files:\n",
    "        resultPredict = classify_images(\"test/\" + dir + \"/\" + file);\n",
    "        if(resultPredict == dir):\n",
    "            countTotalCorrect += 1\n",
    "    countTotalFileTest = countTotalFileTest + len(files)\n",
    "\n",
    "print(\"Accuracy in test samples: \" + str(countTotalCorrect / countTotalFileTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b14cf-a871-4a9b-9839-919ce839fb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
